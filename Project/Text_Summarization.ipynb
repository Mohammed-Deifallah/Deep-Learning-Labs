{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "import string\n",
    "import nltk\n",
    "#nltk.download('stopwords')\n",
    "#nltk.download('punkt')\n",
    "#nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "porter_stemmer = PorterStemmer()\n",
    "wnl = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "\t# open the file as read only\n",
    "\tfile = open(filename, encoding='utf-8')\n",
    "\t# read all text\n",
    "\ttext = file.read()\n",
    "\t# close the file\n",
    "\tfile.close()\n",
    "\treturn text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split a document into news story and highlights\n",
    "def split_story(doc):\n",
    "\t# find first highlight\n",
    "\tindex = doc.find('@highlight')\n",
    "\t# split into story and highlights\n",
    "\tstory, highlights = doc[:index], doc[index:].split('@highlight')\n",
    "\t# strip extra white space around each highlight\n",
    "\thighlights = [h.strip() for h in highlights if len(h) > 0]\n",
    "\treturn story, highlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load all stories in a directory\n",
    "def load_stories(directory):\n",
    "\tall_stories = list()\n",
    "\tfor name in listdir(directory):\n",
    "\t\tfilename = directory + '/' + name\n",
    "\t\t# load document\n",
    "\t\tdoc = load_doc(filename)\n",
    "\t\t# split into story and highlights\n",
    "\t\tstory, highlights = split_story(doc)\n",
    "\t\t# store\n",
    "\t\tall_stories.append({'story':story, 'highlights':highlights})\n",
    "\treturn all_stories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 20\n"
     ]
    }
   ],
   "source": [
    "# load stories\n",
    "directory = 'cnn/examples/'\n",
    "stories = load_stories(directory)\n",
    "print('Loaded Stories %d' % len(stories))\n",
    "stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n",
    "            'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours',\n",
    "            'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she',\n",
    "            \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself',\n",
    "            'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which',\n",
    "            'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am',\n",
    "            'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has',\n",
    "            'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', \n",
    "            'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of',\n",
    "            'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "            'through', 'during', 'before', 'after', 'above', 'below', 'to',\n",
    "            'from', 'up', 'down', 'in', 'out', 'on',\n",
    "            'off', 'over', 'under', 'again', 'further',\n",
    "            'then', 'once', 'here', 'there', 'when', 'where',            \n",
    "            'why', 'how', 'all', 'any', 'both', 'each', 'few', \n",
    "            'more', 'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \n",
    "            's', 't', 'can', 'will', 'just', 'should', \n",
    "            \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highlights: ['NEW: Bermudan premier: \"Above all, this was a humanitarian act\"', 'Uyghurs are native Chinese Muslims; the detainees were apprehended in Pakistan', 'China urges U.S. to hand over all 17 Uyghurs held at Guantanamo Bay, Cuba', 'Official says U.S. still negotiating with Palau to take remaining 13 Uyghurs']\n"
     ]
    }
   ],
   "source": [
    "#print(\"Story: \" + stories[1]['story'])\n",
    "print(\"Highlights: \" + str(stories[1]['highlights']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean a list of lines\n",
    "def clean_lines(lines):\n",
    "\tcleaned = list()\n",
    "\t# prepare a translation table to remove punctuation\n",
    "\ttable = str.maketrans('', '', string.punctuation)\n",
    "\tfor line in lines:\n",
    "\t\t# strip source cnn office if it exists\n",
    "\t\tindex = line.find('(CNN) -- ')\n",
    "\t\tif index > -1:\n",
    "\t\t\tline = line[index+len('(CNN)'):]\n",
    "\t\t# tokenize on white space\n",
    "\t\tline = word_tokenize(line)\n",
    "\t\t# convert to lower case\n",
    "\t\tline = [word.lower() for word in line]\n",
    "\t\t# remove stop words        \n",
    "\t\tline = [word for word in line if word not in stop_words]\n",
    "\t\t# remove punctuation from each token\n",
    "\t\tline = [w.translate(table) for w in line]\n",
    "\t\t# lemmatization\n",
    "\t\tline = [wnl.lemmatize(word) for word in line]\n",
    "\t\t# remove spaces\n",
    "\t\tline = [word for word in line if len(word.strip()) > 0]\n",
    "\t\t# store as string\n",
    "\t\tcleaned.append(' '.join(line))\n",
    "\t# remove empty strings\n",
    "\tcleaned = [c for c in cleaned if len(c) > 0]\n",
    "\treturn cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean stories\n",
    "for example in stories:\n",
    "\texample['story'] = clean_lines(example['story'].split('\\n'))\n",
    "\texample['highlights'] = clean_lines(example['highlights'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save to file\n",
    "from pickle import dump, load\n",
    "dump(stories, open('cnn_dataset.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Stories 20\n"
     ]
    }
   ],
   "source": [
    "# load from file\n",
    "stories = load(open('cnn_dataset.pkl', 'rb'))\n",
    "print('Loaded Stories %d' % len(stories))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['new bermudan premier humanitarian act', 'uyghurs native chinese muslim detainee apprehended pakistan', 'china urge u hand 17 uyghurs held guantanamo bay cuba', 'official say u still negotiating palau take remaining 13 uyghurs']\n"
     ]
    }
   ],
   "source": [
    "print(stories[1]['highlights'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
