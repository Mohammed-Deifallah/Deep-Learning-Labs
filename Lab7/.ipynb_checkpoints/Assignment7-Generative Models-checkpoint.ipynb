{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "cellView": "code",
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "F0ayNvoo4h1C"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pdb\n",
    "\n",
    "# Download the class repository\n",
    "! git clone https://github.com/aamini/introtodeeplearning_labs.git  > /dev/null 2>&1\n",
    "% cd introtodeeplearning_labs \n",
    "! git pull\n",
    "% cd .. \n",
    "\n",
    "!cp -r introtodeeplearning_labs/lab1 lab1\n",
    "!cp -r /content/lab1/util.py util.py\n",
    "\n",
    "# Import the necessary class-specific utility files for this lab\n",
    "import introtodeeplearning_labs as util\n",
    "\n",
    "# Get the training data: both images from CelebA and ImageNet\n",
    "path_to_training_data = tf.keras.utils.get_file('train_face.h5', 'https://www.dropbox.com/s/l5iqduhe0gwxumq/train_face.h5?dl=1')\n",
    "# Instantiate a TrainingDatasetLoader using the downloaded dataset\n",
    "loader = util.TrainingDatasetLoader(path_to_training_data)\n",
    "number_of_training_examples = loader.get_train_size()\n",
    "(images, labels) = loader.get_batch(100)\n",
    "\n",
    "#@title Change the sliders to look at positive and negative training examples! { run: \"auto\" }\n",
    "\n",
    "face_images = images[np.where(labels==1)[0]]\n",
    "not_face_images = images[np.where(labels==0)[0]]\n",
    "\n",
    "idx_face = 16 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "idx_not_face = 42 #@param {type:\"slider\", min:0, max:50, step:1}\n",
    "\n",
    "plt.figure(figsize=(4,2))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(face_images[idx_face])\n",
    "plt.title(\"Face\")\n",
    "plt.grid(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(not_face_images[idx_not_face])\n",
    "plt.title(\"Not Face\")\n",
    "plt.grid(False)\n",
    "\n",
    "ppb = util.PPBFaceEvaluator(skip=4) # create the dataset handler\n",
    "\n",
    "gender = \"male\" #@param [\"male\", \"female\"]\n",
    "skin_color = \"lighter\" #@param [\"lighter\", \"darker\"]\n",
    "\n",
    "img = ppb.get_sample_faces_from_demographic(gender, skin_color)\n",
    "plt.imshow(img)\n",
    "plt.grid(False)\n",
    "\n",
    "n_outputs = 1 # number of outputs (i.e., face or not face)\n",
    "n_filters = 12 # base number of convolutional filters\n",
    "\n",
    "'''Function to define a standard CNN model'''\n",
    "def make_standard_classifier():\n",
    "    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "\n",
    "    model = tf.keras.Sequential([\n",
    "    \t# TODO: define a convolutional layer with n_filters 5x5 filters and 2x2 stride\n",
    "        conv2D(filters=n_filters, kernel_size=[5, 5], stride=[2, 2], input_shape=(64,64,3)),\n",
    "        BatchNormalization(),\n",
    "        \n",
    "        # TODO: define a convolutional layer with 2*n_filters 5x5 filters and 2x2 stride\n",
    "        Conv2D(filters=2*n_filters, kernel_size=[5, 5],  strides=[2, 2]),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # TODO: define a convolutional layer with 4*n_filters 3x3 filters and 2x2 stride\n",
    "        Conv2D(filters=4*n_filters, kernel_size=[3, 3],  strides=[2, 2]),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        # TODO: define a convolutional layer with 6*n_filters 3x3 filters and 1x1 stride\n",
    "        Conv2D(filters=6*n_filters, kernel_size=[3, 3],  strides=[1, 1]),\n",
    "        BatchNormalization(),\n",
    "\n",
    "        Flatten(),\n",
    "        Dense(1, activation=None),\n",
    "        tf.keras.layers.Dropout(0.5)\n",
    "    ])\n",
    "    return model\n",
    "  \n",
    "standard_classifier = make_standard_classifier()\n",
    "\n",
    "batch_size = 36\n",
    "num_epochs = 10  # keep small to run faster\n",
    "learning_rate = 1e-3\n",
    "\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate) # define our optimizer\n",
    "loss_history = util.LossHistory(smoothing_factor=0.99) # to record the evolution of the loss\n",
    "plotter = util.PeriodicPlotter(sec=2, scale='semilogy')\n",
    "\n",
    "# The training loop!\n",
    "for epoch in range(num_epochs):\n",
    "  \n",
    "  custom_msg = util.custom_progress_text(\"Epoch: %(epoch).0f Loss: %(loss)2.2f\")\n",
    "  bar = util.create_progress_bar(custom_msg)\n",
    "  \n",
    "  for idx in bar(range(loader.get_train_size()//batch_size)):\n",
    "    # First grab a batch of training data and convert the input images to tensors\n",
    "    x, y = loader.get_batch(batch_size)\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "    \n",
    "    # GradientTape to record differentiation operations\n",
    "    with tf.GradientTape() as tape:\n",
    "      logits = standard_classifier(x) # feed the images into the model\n",
    "      loss_value = tf.nn.sigmoid_cross_entropy_with_logits(labels=y, logits=logits) # compute the loss\n",
    "\n",
    "    custom_msg.update_mapping(epoch=epoch, loss=loss_value.numpy().mean())\n",
    "    # Backpropagation\n",
    "    grads = tape.gradient(loss_value, standard_classifier.variables)\n",
    "    optimizer.apply_gradients(zip(grads, standard_classifier.variables), global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    loss_history.append(loss_value.numpy().mean()) \n",
    "    plotter.plot(loss_history.get())\n",
    "\n",
    "# Evaluate on a subset of CelebA+Imagenet\n",
    "(batch_x, batch_y) = loader.get_batch(5000)\n",
    "y_pred_standard = tf.round(tf.nn.sigmoid(standard_classifier.predict(batch_x)))\n",
    "acc_standard = tf.reduce_mean(tf.cast(tf.equal(batch_y, y_pred_standard), tf.float32))\n",
    "print \"Standard CNN accuracy on (potentially biased) training set: {:.4f}\".format(acc_standard.numpy())\n",
    "\n",
    "# Evaluate on PPB dataset (takes ~3 minutes)\n",
    "standard_cnn_accuracy = []\n",
    "for skin_color in ['lighter', 'darker']:\n",
    "  for gender in ['male', 'female']:\n",
    "    standard_cnn_accuracy.append( ppb.evaluate([standard_classifier], gender, skin_color, from_logit=True)[0] )\n",
    "    print \n",
    "    print \"{} {}: {}\".format(gender, skin_color, standard_cnn_accuracy[-1])\n",
    "    \n",
    "plt.bar(range(4), standard_cnn_accuracy)\n",
    "plt.xticks(range(4), ('LM', 'LF', 'DM', 'DF'))\n",
    "plt.ylim(np.min(standard_cnn_accuracy)-0.1,np.max(standard_cnn_accuracy)+0.1)\n",
    "plt.ylabel('Accuracy')\n",
    "\n",
    "# Function to calculate VAE loss given an input x, reconstructed output x_pred, \n",
    "#    encoded means mu, encoded log of standard deviation logsigma, and weight parameter for the latent loss\n",
    "def vae_loss_function(x, x_pred, mu, logsigma, kl_weight=0.0005):\n",
    "  '''TODO: Define the latent loss'''\n",
    "  latent_loss = 0.5 * tf.reduce_sum(tf.exp(logsigma) + tf.square(mu) - 1.0 - logsigma, axis=1)\n",
    "\n",
    "  '''TODO: Define the reconstruction loss. Hint: you'll need to use tf.reduce_mean'''\n",
    "\n",
    "  '''TODO: Define the VAE loss'''\n",
    "\n",
    "  return vae_loss\n",
    "\n",
    "\n",
    "\"\"\"Reparameterization trick by sampling from an isotropic unit Gaussian.\n",
    "# Arguments\n",
    "    args (tensor): mean and log of standard deviation of latent distribution (Q(z|X))\n",
    "# Returns\n",
    "    z (tensor): sampled latent vector\n",
    "\"\"\"\n",
    "def sampling(args):\n",
    "    z_mean, z_logsigma = args\n",
    "    batch = z_mean.shape[0]\n",
    "    dim = z_mean.shape[1]\n",
    "    \n",
    "    # by default, random_normal has mean=0 and std=1.0\n",
    "    epsilon = tf.random_normal(tf.shape(z_mean))\n",
    "    '''TODO: Define the reparameterization computation!'''\n",
    "\n",
    "# Loss function for DB-VAE\n",
    "def debiasing_loss_function(x, x_pred, y, y_logit, mu, logsigma):\n",
    "\n",
    "  '''TODO: call the relevant function to obtain VAE loss'''\n",
    "\n",
    "  '''TODO: define the classification loss'''\n",
    "  \n",
    "  # Use the training data labels to create variable face_mask\n",
    "  face_mask = tf.cast(tf.equal(y, 1), tf.float32)\n",
    "  \n",
    "  '''TODO: define the DB-VAE total loss! Hint: think about the dimensionality of your output.'''\n",
    "  \n",
    "  return total_loss, classification_loss\n",
    "\n",
    "  latent_dim = 100\n",
    "\n",
    "'''Define the encoder network for the DB-VAE'''\n",
    "def make_face_encoder_network():\n",
    "    Conv2D = functools.partial(tf.keras.layers.Conv2D, padding='same', activation='relu')\n",
    "    BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "    Flatten = tf.keras.layers.Flatten\n",
    "    Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "\n",
    "    inputs = tf.keras.layers.Input(shape=(64,64,3))\n",
    "    \n",
    "    hidden = Conv2D(filters=1*n_filters, kernel_size=[5,5],  strides=[2,2])(inputs)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Conv2D(filters=2*n_filters, kernel_size=[5,5],  strides=[2,2])(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Conv2D(filters=4*n_filters, kernel_size=[3,3],  strides=[2,2])(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "    hidden = Conv2D(filters=6*n_filters, kernel_size=[3,3],  strides=[1,1])(hidden)\n",
    "    hidden = BatchNormalization()(hidden)\n",
    "\n",
    "    hidden = Flatten(name='flatten')(hidden)\n",
    "    \n",
    "    '''Encoder outputs:\n",
    "        y_logit: supervised class prediction\n",
    "        z_mean: means in the latent space\n",
    "        z_logsigma: standard deviations in the latent space'''\n",
    "    y_logit = Dense(1, activation=None, name='y_logit')(hidden)\n",
    "    z_mean = Dense(latent_dim, name='z_mean')(hidden)\n",
    "    z_logsigma = Dense(latent_dim, name='z_logsigma')(hidden)\n",
    "\n",
    "    # use reparameterization trick to sample from the latent space\n",
    "    z = tf.keras.layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_logsigma])\n",
    "\n",
    "    # define the outputs that the encoder model should return\n",
    "    outputs = [y_logit, z_mean, z_logsigma, z]\n",
    "    # finalize the encoder model\n",
    "    encoder = tf.keras.Model(inputs=inputs, outputs=outputs, name='encoder')\n",
    "\n",
    "    # get the shape of the final convolutional output (right before the flatten)\n",
    "    flatten_layer_idx = encoder.layers.index(encoder.get_layer('flatten'))\n",
    "    pre_flatten_shape = encoder.layers[flatten_layer_idx-1].get_output_at(0).shape[1:]\n",
    "    \n",
    "    return encoder, inputs, outputs, pre_flatten_shape\n",
    "\n",
    " '''Define the decoder network for the DB-VAE'''\n",
    "def make_face_decoder_network(pre_flatten_shape):\n",
    "  Conv2DTranspose = functools.partial(tf.keras.layers.Conv2DTranspose, padding='same', activation='relu')\n",
    "  BatchNormalization = tf.keras.layers.BatchNormalization\n",
    "  Flatten = tf.keras.layers.Flatten\n",
    "  Dense = functools.partial(tf.keras.layers.Dense, activation='relu')\n",
    "\n",
    "  latent_inputs = tf.keras.layers.Input(shape=(latent_dim,))\n",
    "  \n",
    "  hidden = Dense(tf.reduce_prod(pre_flatten_shape))(latent_inputs)\n",
    "  hidden = tf.keras.layers.Reshape(pre_flatten_shape)(hidden)\n",
    "  \n",
    "  # series of deconvolutional layers with batch normalization\n",
    "  hidden = Conv2DTranspose(filters=4*n_filters, kernel_size=[3,3],  strides=[1,1])(hidden)\n",
    "  hidden = BatchNormalization()(hidden)\n",
    "  hidden = Conv2DTranspose(filters=2*n_filters, kernel_size=[3,3],  strides=[2,2])(hidden)\n",
    "  hidden = BatchNormalization()(hidden)\n",
    "  hidden = Conv2DTranspose(filters=1*n_filters, kernel_size=[5,5],  strides=[2,2])(hidden)\n",
    "  hidden = BatchNormalization()(hidden)\n",
    "  \n",
    "  x_hat = Conv2DTranspose(filters=3, kernel_size=[5,5], strides=[2,2])(hidden)\n",
    "\n",
    "  # instantiate decoder model\n",
    "  decoder = tf.keras.Model(inputs=latent_inputs, outputs=x_hat, name='decoder')\n",
    "  return decoder\n",
    "\n",
    "  '''TODO: create the encoder and decoder networks'''\n",
    "\n",
    "# initialize the models\n",
    "encoder_output = encoder(inputs)\n",
    "y_logit, z_mean, z_logsigma, z = encoder_output\n",
    "reconstructed_inputs = decoder(z)\n",
    "\n",
    "vae = tf.keras.Model(inputs, reconstructed_inputs)\n",
    "util.display_model(encoder)\n",
    "\n",
    "# Function to return the means for an input image batch\n",
    "def get_latent_mu(images, encoder, batch_size=1024):\n",
    "    N = images.shape[0]\n",
    "    mu = np.zeros((N, latent_dim))\n",
    "    for start_ind in xrange(0, N, batch_size):\n",
    "        end_ind = min(start_ind+batch_size, N+1)\n",
    "        batch = images[start_ind:end_ind]\n",
    "        batch = tf.convert_to_tensor(batch, dtype=tf.float32)/255.\n",
    "        _, batch_mu, _, _ = encoder(batch)\n",
    "        mu[start_ind:end_ind] = batch_mu\n",
    "    return mu\n",
    "  \n",
    "'''Function that recomputes the sampling probabilities for images within a batch\n",
    "    based on how they distribute across the '''\n",
    "def get_training_sample_probabilities(images, encoder, bins=10, smoothing_fac=0.0): \n",
    "    print \"Recomputing the sampling probabilities\"\n",
    "    \n",
    "    mu = get_latent_mu(images, encoder)\n",
    "    # sampling probabilities for the images\n",
    "    training_sample_p = np.zeros(mu.shape[0])\n",
    "    \n",
    "    # consider the distribution for each latent variable \n",
    "    for i in range(latent_dim):\n",
    "      \n",
    "        latent_distribution = mu[:,i]\n",
    "        # generate a histogram of the latent distribution\n",
    "        hist_density, bin_edges =  np.histogram(latent_distribution, density=True, bins=bins)\n",
    "\n",
    "        # find which latent bin every data sample falls in \n",
    "        # https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.digitize.html\n",
    "        bin_edges[0] = -float('inf')\n",
    "        bin_edges[-1] = float('inf')\n",
    "        '''TODO: call the digitize function to find which bins in the latent distribution \n",
    "            every data sample falls in to'''\n",
    "\n",
    "        # smooth the density function [Eq. #]\n",
    "        hist_smoothed_density = hist_density + smoothing_fac\n",
    "        hist_smoothed_density = hist_smoothed_density / np.sum(hist_smoothed_density)\n",
    "\n",
    "        '''TODO: invert the density function to compute the sampling probability!\n",
    "            HINT: think carefully about the indexing of the bins! What is the length of bin_edges?'''\n",
    "        \n",
    "        # normalize all probabilities\n",
    "        p = p / np.sum(p)\n",
    "        \n",
    "        # update sampling probabilities \n",
    "        training_sample_p = np.maximum(p, training_sample_p)\n",
    "        \n",
    "    # final normalization\n",
    "    training_sample_p /= np.sum(training_sample_p)\n",
    "\n",
    "    return training_sample_p\n",
    "\n",
    "loss_history = []\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "\n",
    "enable_debiasing = True\n",
    "all_faces = loader.get_all_train_faces() # parameter from data loader\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "  \n",
    "  # progress message and bar\n",
    "  custom_msg = util.custom_progress_text(\"Epoch: %(epoch).0f   Iter: %(idx).0f   Class Loss: %(class_loss)2.2f   Loss: %(loss)2.2f\")\n",
    "  bar = util.create_progress_bar(custom_msg)\n",
    "\n",
    "  p_faces = None\n",
    "  if enable_debiasing: \n",
    "      # Recompute data sampling proabilities if debiasing is enabled\n",
    "      '''TODO: write the function call to recompute the sampling probabilities\n",
    "          when debiasing is enabled'''\n",
    "  \n",
    "  for idx in bar(range(loader.get_train_size()//batch_size)):\n",
    "    # load a batch of data\n",
    "    (x, y) = loader.get_batch(batch_size, p_pos=p_faces)\n",
    "    x = tf.convert_to_tensor(x, dtype=tf.float32)\n",
    "    y = tf.convert_to_tensor(y, dtype=tf.float32)\n",
    "  \n",
    "    # define GradientTape for automatic differentiation\n",
    "    with tf.GradientTape() as tape:\n",
    "      y_logit, mu, logsigma, z = encoder(x)\n",
    "      x_hat = decoder(z)\n",
    "      '''TODO: call the relevant loss function to compute the loss'''\n",
    "    \n",
    "    '''TODO: use the GradientTape.gradient method to compute the gradients'''\n",
    "\n",
    "    # apply gradients to variables\n",
    "    optimizer.apply_gradients(zip(grads, vae.variables),\n",
    "                              global_step=tf.train.get_or_create_global_step())\n",
    "\n",
    "    # track the losses\n",
    "    class_loss_value = class_loss.numpy().mean()\n",
    "    loss_value = loss.numpy().mean()\n",
    "    loss_history.append((class_loss_value, loss_value))\n",
    "    custom_msg.update_mapping(epoch=epoch, idx=idx, loss=loss_value, class_loss=class_loss_value)\n",
    "    \n",
    "    # plot the progress every 100 steps\n",
    "    if idx%100 == 0: \n",
    "      util.plot_sample(x,y,vae)\n",
    "\n",
    "# Evaluate on PPB dataset (takes ~4 minutes)\n",
    "accuracy_debiased = []\n",
    "for skin_color in ['lighter', 'darker']:\n",
    "  for gender in ['male', 'female']:\n",
    "    accuracy_debiased.append( ppb.evaluate([encoder], gender, skin_color, output_idx=0, from_logit=True)[0] )\n",
    "    print \n",
    "    print \"{} {}: {}\".format(gender, skin_color, accuracy_debiased[-1])\n",
    "    \n",
    "    \n",
    "bar_width = 0.3\n",
    "plt.bar(np.arange(4), standard_cnn_accuracy, width=bar_width)\n",
    "plt.bar(np.arange(4)+bar_width, accuracy_debiased, width=bar_width)\n",
    "plt.legend(('Standard Classifier','Debiased Classifier (DB-VAE)'))\n",
    "plt.xticks(np.arange(4), ('LM', 'LF', 'DM', 'DF'))\n",
    "plt.ylim(np.min([standard_cnn_accuracy,accuracy_debiased])-0.1,1)\n",
    "plt.ylabel('Accuracy')\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Assignment7-Generative Models.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 2",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
